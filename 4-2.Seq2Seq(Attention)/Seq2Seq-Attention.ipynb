{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0766e3-3816-4303-af74-2dfc50ba5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mindspore\n",
    "import mindspore.nn as nn\n",
    "import mindspore.numpy as mnp\n",
    "import mindspore.ops as ops\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4888e33-7ea9-437a-be7d-3151891ed97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S: Symbol that shows starting of decoding input\n",
    "# E: Symbol that shows starting of decoding output\n",
    "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
    "\n",
    "def make_batch():\n",
    "    input_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[0].split()]]]\n",
    "    output_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[1].split()]]]\n",
    "    target_batch = [[word_dict[n] for n in sentences[2].split()]]\n",
    "\n",
    "    # make tensor\n",
    "    return mindspore.Tensor(input_batch), mindspore.Tensor(output_batch), mindspore.Tensor(target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0242cf52-c02a-4670-ab34-e376cb140744",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "\n",
    "        # Linear for attention\n",
    "        self.attn = nn.Dense(n_hidden, n_hidden)\n",
    "        self.out = nn.Dense(n_hidden * 2, n_class)\n",
    "\n",
    "    def construct(self, enc_inputs, hidden, dec_inputs):\n",
    "        enc_inputs = enc_inputs.swapaxes(0, 1)  # enc_inputs: [n_step(=n_step, time step), batch_size, n_class]\n",
    "        dec_inputs = dec_inputs.swapaxes(0, 1)  # dec_inputs: [n_step(=n_step, time step), batch_size, n_class]\n",
    "\n",
    "        # enc_outputs : [n_step, batch_size, num_directions(=1) * n_hidden], matrix F\n",
    "        # enc_hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "        enc_outputs, enc_hidden = self.enc_cell(enc_inputs, hidden)\n",
    "\n",
    "        trained_attn = []\n",
    "        hidden = enc_hidden\n",
    "        n_step = len(dec_inputs)\n",
    "        model = mnp.ones([n_step, 1, n_class])\n",
    "\n",
    "        for i in range(n_step):  # each time step\n",
    "            # dec_output : [n_step(=1), batch_size(=1), num_directions(=1) * n_hidden]\n",
    "            # hidden : [num_layers(=1) * num_directions(=1), batch_size(=1), n_hidden]\n",
    "            dec_output, hidden = self.dec_cell(dec_inputs[i].expand_dims(0), hidden)\n",
    "            attn_weights = self.get_att_weight(dec_output, enc_outputs)  # attn_weights : [1, 1, n_step]\n",
    "            trained_attn.append(attn_weights.squeeze())\n",
    "\n",
    "            # matrix-matrix product of matrices [1,1,n_step] x [1,n_step,n_hidden] = [1,1,n_hidden]\n",
    "            context = mnp.matmul(attn_weights, enc_outputs.swapaxes(0, 1))\n",
    "            dec_output = dec_output.squeeze(0)  # dec_output : [batch_size(=1), num_directions(=1) * n_hidden]\n",
    "            context = context.squeeze(1)  # [1, num_directions(=1) * n_hidden]\n",
    "            out = self.out(mnp.concatenate((dec_output, context), 1))\n",
    "            model[i] = out\n",
    "\n",
    "        # make model shape [n_step, n_class]\n",
    "        return model.swapaxes(0, 1).squeeze(0), trained_attn\n",
    "\n",
    "    def get_att_weight(self, dec_output, enc_outputs):  # get attention weight one 'dec_output' with 'enc_outputs'\n",
    "        n_step = len(enc_outputs)\n",
    "        attn_scores = mnp.zeros(n_step)  # attn_scores : [n_step]\n",
    "\n",
    "        for i in range(n_step):\n",
    "            attn_scores[i] = self.get_att_score(dec_output, enc_outputs[i])\n",
    "\n",
    "        # Normalize scores to weights in range 0 to 1\n",
    "        return ops.Softmax()(attn_scores).view(1, 1, -1)\n",
    "\n",
    "    def get_att_score(self, dec_output, enc_output):  # enc_outputs [batch_size, num_directions(=1) * n_hidden]\n",
    "        score = self.attn(enc_output)  # score : [batch_size, n_hidden]\n",
    "        return mnp.dot(dec_output.view(-1), score.view(-1))  # inner product make scalar value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b408b5-5726-404b-a552-3dfd1171db48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WithLossCell(nn.Cell):\n",
    "    def __init__(self, backbone, loss_fn):\n",
    "        super(WithLossCell, self).__init__(auto_prefix=False)\n",
    "        self._backbone = backbone\n",
    "        self._loss_fn = loss_fn\n",
    "\n",
    "    def construct(self, *args):\n",
    "        out, _ = self._backbone(*args[:-1])\n",
    "        return self._loss_fn(out.view(-1, out.shape[-1]), args[-1].view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad18877e-322a-42af-97bf-f13b561760d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step = 5 # number of cells(= number of Step)\n",
    "n_hidden = 128 # number of hidden units in one cell\n",
    "\n",
    "sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']\n",
    "\n",
    "word_list = \" \".join(sentences).split()\n",
    "word_list = list(set(word_list))\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "number_dict = {i: w for i, w in enumerate(word_list)}\n",
    "n_class = len(word_dict)  # vocab list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef50a8e-f4c0-44c8-82cd-750cc7ba3d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Attention()\n",
    "criterion = nn.SoftmaxCrossEntropyWithLogits(True, 'mean')\n",
    "network = WithLossCell(model, criterion)\n",
    "optimizer = nn.Adam(model.trainable_params(), learning_rate=0.001)\n",
    "train_network = nn.TrainOneStepCell(network, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ce97a-4fc3-4953-b641-bcd9f800f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "hidden = mnp.zeros((1, 1, n_hidden))\n",
    "\n",
    "input_batch, output_batch, target_batch = make_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f192fcb-4bad-4b97-9611-e24c8a0d966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "for epoch in range(2000):\n",
    "    loss = train_network(input_batch, hidden, output_batch, target_batch.squeeze(0))\n",
    "    if (epoch + 1) % 400 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss.asnumpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa47c242-103d-471d-983d-2480830b7d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "test_batch = [np.eye(n_class)[[word_dict[n] for n in 'SPPPP']]]\n",
    "test_batch = mindspore.Tensor(test_batch)\n",
    "predict, trained_attn = model(input_batch, hidden, test_batch)\n",
    "predict = predict.argmax(1)\n",
    "print(sentences[0], '->', [number_dict[int(n.asnumpy())] for n in predict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc60f4c8-461b-4640-97f2-b50fb6da5f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Attention\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.matshow([attn.asnumpy() for attn in trained_attn], cmap='viridis')\n",
    "ax.set_xticklabels([''] + sentences[0].split(), fontdict={'fontsize': 14})\n",
    "ax.set_yticklabels([''] + sentences[2].split(), fontdict={'fontsize': 14})\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
