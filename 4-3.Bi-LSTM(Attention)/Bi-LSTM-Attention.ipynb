{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b3f0a-f9a8-48bc-8aa9-eb6970040e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mindspore\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "import mindspore.numpy as mnp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4956c4aa-bf84-44c0-b271-58a26a26ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_Attention(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(BiLSTM_Attention, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, n_hidden, bidirectional=True)\n",
    "        self.out = nn.Dense(n_hidden * 2, num_classes)\n",
    "\n",
    "    # lstm_output : [batch_size, n_step, n_hidden * num_directions(=2)], F matrix\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        hidden = final_state.view(-1, n_hidden * 2, 1)   # hidden : [batch_size, n_hidden * num_directions(=2), 1(=n_layer)]\n",
    "        attn_weights = mnp.matmul(lstm_output, hidden).squeeze(2) # attn_weights : [batch_size, n_step]\n",
    "        soft_attn_weights = ops.Softmax(1)(attn_weights)\n",
    "        # [batch_size, n_hidden * num_directions(=2), n_step] * [batch_size, n_step, 1] = [batch_size, n_hidden * num_directions(=2), 1]\n",
    "        context = mnp.matmul(lstm_output.swapaxes(1, 2), soft_attn_weights.expand_dims(2)).squeeze(2)\n",
    "        return context, soft_attn_weights # context : [batch_size, n_hidden * num_directions(=2)]\n",
    "\n",
    "    def construct(self, X):\n",
    "        input = self.embedding(X) # input : [batch_size, len_seq, embedding_dim]\n",
    "        input = input.transpose(1, 0, 2) # input : [len_seq, batch_size, embedding_dim]\n",
    "\n",
    "        hidden_state = mnp.zeros([1*2, len(X), n_hidden]) # [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
    "        cell_state = mnp.zeros([1*2, len(X), n_hidden]) # [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
    "\n",
    "        # final_hidden_state, final_cell_state : [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(input, (hidden_state, cell_state))\n",
    "        output = output.transpose(1, 0, 2) # output : [batch_size, len_seq, n_hidden]\n",
    "        attn_output, attention = self.attention_net(output, final_hidden_state)\n",
    "        return self.out(attn_output), attention # model : [batch_size, num_classes], attention : [batch_size, n_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6ae8d0-6a59-42c4-a867-1a2cc0e74ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WithLossCell(nn.Cell):\n",
    "    def __init__(self, backbone, loss_fn):\n",
    "        super(WithLossCell, self).__init__(auto_prefix=False)\n",
    "        self._backbone = backbone\n",
    "        self._loss_fn = loss_fn\n",
    "\n",
    "    def construct(self, *args):\n",
    "        out, _ = self._backbone(*args[:-1])\n",
    "        return self._loss_fn(out.view(-1, out.shape[-1]), args[-1].view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e4fe2-0dd5-4473-bbd4-67f2115bd181",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 2 # embedding size\n",
    "n_hidden = 5  # number of hidden units in one cell\n",
    "num_classes = 2  # 0 or 1\n",
    "\n",
    "# 3 words sentences (=sequence_length is 3)\n",
    "sentences = [\"i love you\", \"he loves me\", \"she likes baseball\", \"i hate you\", \"sorry for that\", \"this is awful\"]\n",
    "labels = [1, 1, 1, 0, 0, 0]  # 1 is good, 0 is not good.\n",
    "\n",
    "word_list = \" \".join(sentences).split()\n",
    "word_list = list(set(word_list))\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "vocab_size = len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d10505-d45e-481f-a406-e66c42b15775",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM_Attention()\n",
    "criterion = nn.SoftmaxCrossEntropyWithLogits(True, 'mean')\n",
    "network = WithLossCell(model, criterion)\n",
    "optimizer = nn.Adam(model.trainable_params(), learning_rate=0.001)\n",
    "train_network = nn.TrainOneStepCell(network, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c877f4-d0ab-48d5-a724-bff9acc7527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = mindspore.Tensor([np.asarray([word_dict[n] for n in sen.split()]) for sen in sentences])\n",
    "targets = mindspore.Tensor([out for out in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79044130-6d93-41cc-b4a4-faf67858be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "for epoch in range(5000):\n",
    "    loss = train_network(inputs, targets)\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss.asnumpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29d2248-796b-44f5-890e-2ec9267a4de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "test_text = 'sorry hate you'\n",
    "tests = [np.asarray([word_dict[n] for n in test_text.split()])]\n",
    "test_batch = mindspore.Tensor(tests)\n",
    "\n",
    "# Predict\n",
    "predict, attention = model(test_batch)\n",
    "predict = predict.argmax(1)\n",
    "\n",
    "if predict[0] == 0:\n",
    "    print(test_text,\"is Bad Mean...\")\n",
    "else:\n",
    "    print(test_text,\"is Good Mean!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2e17c5-2f00-4673-a2f7-b91cfa7e4c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 3)) # [batch_size, n_step]\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.matshow(attention.asnumpy(), cmap='viridis')\n",
    "ax.set_xticklabels(['']+['first_word', 'second_word', 'third_word'], fontdict={'fontsize': 14}, rotation=90)\n",
    "ax.set_yticklabels(['']+['batch_1', 'batch_2', 'batch_3', 'batch_4', 'batch_5', 'batch_6'], fontdict={'fontsize': 14})\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
